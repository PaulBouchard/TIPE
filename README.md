# TIPE

Introduction :

Mon intérêt premier dans les prothèses mécaniques m’a permit de m’orienter par la suite sur la prédiction de mouvements selon les ondes myoélectriques dans les muscles ; pour finalement me focaliser sur  la prédiction de diagnostic d’un électrocardiogramme. Le but final étant d’apporter une aide aux médecins dans leur diagnostic, ce TIPE est ainsi focalisé sur la santé du patient et la prévention de maladies chez ce dernier. Dans l’optique de créer un programme de machine learning sous python et de comparer des placements d’électrodes, on a tout d’abord besoin d’une base de données qui puisse être manipulée grâce à python et qui comporte aussi des informations sur les placements faits.

I.1 - Établir une base de données d'ECG:

1) Définition:

Une base de données permet de stocker et de retrouver des données structurées ou de l’information, souvent en rapport avec un thème ou une activité ; celles-ci peuvent être de natures différentes et plus ou moins reliées entre elles. Dans une base de données dites relationnelle, les données sont organisées dans des tableaux pour mieux les retrouver. Par exemple, le langage SQL permet de les manipuler, et voici un exemple de possibles colonnes d’une base de données d’ECG : des informations personnelles sur le patient, les valeurs d’ECG obtenues et leur interprétation.

2) Justification:

Étant donné que l’on peut y stocker des mots ou des valeurs, la base de données semble parfaite pour caractériser un résultat d’électrocardiogramme, ce dernier pouvant être décomposé selon différents paramètres. Par exemple, pour un algorithme focalisé sur la détection de tachycardie, l’un des paramètres comparatifs serait celui de la fréquence de l’onde P. Aussi, le but étant de comparer différents résultats entre eux pour en ressortir une prédiction sur un nouvel échantillon, la base de données permet justement de stocker autant de résultats d’ECG que l’on veut. Une base de donnée pouvant être représentée comme un tableau selon certains fichiers, on pourra alors en manipuler les éléments comme on le fait avec un type array ou DataFrame sous python.

3) Recherche de la base et justification:

Afin de ne pas s’égarer dans des ensembles de données trop « diverse », on cherche une base de données recensant possiblement de simples résultats d’ECG ainsi que les caractéristiques propres à ces derniers. Le site Physionet recense un large choix de bases de données médicales. Parmi toutes, on réussit à y trouver une base de données : celle nommée PTB-XL. Ces ECG ont été fait dans des conditions normales et on peut avoir accès à chacun d’entre eux individuellement dans des fichiers de type DAT qui regroupe les données obtenues. Toutes les données utiles ou importantes ont été regroupé dans un fichier csv qui est un type de fichier manipulable dans python. La PTB-XL se démarque des autres par le nombre d’ECG qu’elle contient (plus de 20000) et également par le fait qu’elle recense exclusivement des ECG « normaux » là où les autres donnent accès à des ECG très unique car fait dans des conditions spéciales (position foetal, après ingestion de substances, femmes enceintes,…). La présence du fichier csv est aussi là pour aider les personnes voulant faire du machine learning sur des ECG. La PTB-XL semble ainsi la base de données la plus adaptée pour notre futur programme.

4) Présentation de la base de données :

La PTB-XL recense 21837 ECG cliniques à 12 dérivations sur 18885 patients, tous d'une durée de 10 secondes. On va principalement utiliser le fichier csv étant donné qu’il nous donne toutes les informations dont on a besoin. Il apparaît sous la forme d’un tableau de 21 837 lignes par 28 colonnes. Une ligne par ECG, une colonne par informations importantes. Les premières colonnes contiennent des informations sur les patients (âge,sexe,taille,masse,…) ; ensuite viennent les relevés d’ECG sous forme de rapport général et d’informations sur l’axe du cœur, la validation par machine et par médecin ; et enfin les données sur les signaux eux mêmes avec les interférences et les problèmes d’électrodes. La colonne strat_fold indique la qualité de l’ECG par rapport aux validations faite. Un second tableau est fourni et indique les significations de certains sigles inscrits dans une colonne de rapport. Également, le programme fourni permet d’extraire les données brutes des ECG, ici on peut voir l’exemple du tout premier ECG avec d’un côté la représentation graphique du signal en µV par secondes et de l’autre les premières valeures acquises en µV. Chaque ligne correspond à un électrode différent (V1,V2,V3,etc) et chaque colonne est une mesure prise au cours du temps. La fréquence d’échantillonnage étant de 100HZ ici, on a évidemment 1000 colonnes car l’ECG dure 10 secondes. Pour mieux visualiser, on prend le premier dixième de seconde de ce qu’à enregistré la dérivation I.

I.2 - Comparer les différentes méthodes de placements d'électrodes:

1) Définition :

Lors d’un ECG, afin d’enregistrer l’activité électrique du cœur, on utilise des électrodes. Il en existe plusieurs types selon l’usage. La qualité des signaux captés dépend principalement des électrodes : des parasites diminuent la clarté de l’ECG et peuvent être supprimés en utilisant des électrodes de qualité. On les pose sur le corps du patient sur divers emplacements qui peuvent varier selon la méthode utilisée. Voici deux exemples d’électrodes à usage unique, un gel est fourni et on les relie à l’électrocardiographe via des électrodes de monitorage soit des câbles.

2) Présentation :

De nos jours, il n’y a que quelques méthodes utilisées : ECG à 1/3/12 dérivations,ECG vectoriel,etc. Des dérivations sont des ensembles de deux électrodes qu’on place de part et d’autre du cœur afin d’explorer ce dernier selon différents plans. La plupart du temps, il y a une électrode de référence et une autre dite exploratrice qui va capter le potentiel électrique. Donc la dérivation est la mesure d’une différence de potentiel entre deux électrodes. L’ECG à 12 dérivations comporte 6 dérivations directement autour du cœur (V1,V2,V3,…) et 6 autres aux extrémités du corps (I,II,III,aVR,aVL,aVF) et est réalisé à l’aide de 10 électrodes. On a donc 6 signaux enregistrés. Les différences majeures entre les méthodes à dérivations sont la durée d’utilisation des électrodes ou encore la reconnaissance de certaines maladies. La méthode à  12 dérivations est réputée d’être une des meilleures de par le fait que contrairement aux autres, elle puisse diagnostiquer plus de maladies cardiaques. Par exemple, les ischémies ne peuvent être détectées par les ECG à 1/3 dérivations. Par conséquent, la méthode à préconiser pour prédire un diagnostic selon d’anciens résultats est celle-ci.

3) Choix/justification dans PTB-XL

En plus des précédents avantages énoncés, notre  base de données fournit des ECG à 12 dérivations soit la meilleure méthode. Les électrodes de référence sont situés sur le bras droit du patient. Les données brutes ont été enregistrées et stockées dans les fichiers mis à disposition dans records100 et records500 :  donnant accès à des ondes avec une fréquence d’échantillonnage de 500Hz ou 100Hz  Physionet permet de visualiser la forme des ondes, ainsi on peut constater les différences entre les ECG où il y a un problème d’électrodes et ceux où il n’y en a pas.   Ces potentiels problèmes sont indiqués dans le tableau dans les colonnes « electrodes_problems », « static_noise », « burst_noise ».

II - Développer un algorithme de machine learning utilisant la base de données et les résultats comparateurs de méthodes:

1) Définition :

Le machine learning ou l’apprentissage automatique est une technique d’I,A permettant à un ordinateur de prédire certaines choses en lui apprenant à reconnaître des paterns entre des données. Dans notre cas, notre programme utilisera une classification supervisée : c’est-à-dire que l’apprentissage se fait à partir de données annotés et le but est de classer ces dernières dans des catégories. On va utiliser les modèles de régression logistique, arbre de décision et machines à vecteur de support, sans sélection particulière.

2) Régression logistique :

La régression logistique permet de calculer la probabilité qu’un événement se produise ou pas (dans notre cas, présence d’une maladie ou non) en étudiant les relations entre un ensemble de variables x et une variable unique y. Ici, on a affaire à une régression logistique binaire car y E {0,1}. La probabilité se traduit par une fonction notée P(X) avec X un vecteur contenant des variables explicatives, on note B un vecteur contenant les variables que l’on cherche à calculer pour obtenir la fonction de prédiction. BX représente la somme des différents prédicteurs. Et P(X) > 0 si y=1(resp. P(X) < 0 si y=0). La fonction qui se rapproche le plus de cette description est P(X)=1/(1+exp(-BX)). La valeur que renvoie cette fonction est donc la probabilité que l’événement se produise(y=1). Ce modèle est réputé pour être l’un des plus abordables. En python, il suffit d’appeler la fonction nommée LogisticRegression pour pouvoir réaliser une régression logistique.

3) Arbre de décision :

Un arbre de décision modélise une succession de tests pour prédire quelque chose. Ces tests sont des règles de décision qui sont déduites des unes des autres, le tout depuis les données d’entraînement initiales. L’ensemble des choix apparaît donc sous l’apparence d’un arbre. Par exemple, le premier scindement se fait selon la meilleure séparation des variables et on répète cela jusqu’à ce que ça s’arrête. Les décisions possibles sont situées sur les feuilles de l'arbre aux extrémités des branches. Le score Gini qui apparaît dans les cases est une valeur comprise entre 0 et 1 indiquant si la prise de décision est bonne ou non c’est-à-dire si la classe choisie l’est ou pas. Chaque nouvelle branche créée l’est car elle a le score Gini maximal. Sa formule est Gini = somme des Pk*(1-Pk) avec Pk la probabilité d’obtenir la classe k ( dans notre cas, il y a maladie ou non). Dans notre cas, on utilisera un arbre de classification : il nous prédira la classe de la variable de sortie. Voici 2 arbres de décisions correspondant aux programmes que j’ai fait, j’expliquerais donc leur obtention plus tard. C’est la fonction DecsionTreeClassifier qu’on utilisera dans Python.

4) Machines à vecteurs de support :

Les machines à vecteurs de supports ou SVM sont des modèles prédictifs similaires à la régression dans le sens où on les utilise pour prédire la valeur d’une variable en construisant une fonction prenant un vecteur des données pour argument. Si la fonction est supérieur ou égale à 0, alors le vecteur est de classe 1, dans l’autre cas il est de classe 0. Le modèle va apprendre cette fonction à partir de l’ensemble des classes (valeurs à prédire 1 ou 0) associés aux données. La « racine » de la fonction est un hyperplan qui sépare donc les données en deux classes. Les données les plus proches de ce dernier sont nommées vecteurs de support. Cette méthode est très utilisée de par sa simplicité d’utilisation et sa capacité à traiter de grandes bases de données. Dans Python, on utilise la fonction SVM().

5) Programme v1 : 

J’ai d’abord écrit deux programmes pour la régression logistique. Les différences entre les deux se situant dans la partie manipulation de la base de données, j’ai simplement eu à adapter les parties machine learning pour les deux autres méthodes. Tout d’abord, le fichier csv est affecté à une variable Y puis l’on y rajoute une colonne diagnostic qui nous indique la présence ou non d’une maladie chez le patient à l’aide d’un programme fourni dans la base de données. Étant donné que l’on a besoin d’un tableau composé uniquement de valeurs numériques, on retire les colonnes comportant du texte qui ne peut être modifié ainsi que les colonnes quasiment vide. La colonne de diagnostic est modifié en changeant tout ce qui est normal [‘NORM’] par 0 et le reste par 1 grâce à une boucle for et un test if pour chaque case diagnostic. On utilise quasiment la même technique pour modifier les colonnes ‘second_opinion’,’initial_auto..’ et ‘validated_by_human’ en changeant les True et False par 0 ou 1. Afin de modifier la colonne ‘device’, on collecte les différents éléments composant la colonne dans une liste puis on les remplace par l’indice de leur place dans la liste, la nouvelle colonne contient donc des entiers de 1 à 10. On retire maintenant toutes les lignes comportant une case vide à l’aide de la fonction .dropna(). Cette fonction recolle les lignes restantes entre elles sans changer l’index donc afin d’avoir plus de clarté au niveau de ce dernier, on crée un nouveau fichier csv avec tout ce qui nous reste. On retire la colonne diagnostic et on l’affecte à une variable à part. Afin d’entraîner la machine, on scinde notre nouveau tableau et la colonne en deux parties chacuns grâce à la fonction train_test_split() : une d’entraînement et une de test. A partir de là on charge notre méthode de machine learning et on entraîne notre machine grâce à la fonction .fit() avec les sets d’entraînement. Puis on peut faire notre prédiction avec la fonction .predict() associée à notre modèle et notre set de test. Pour déterminer la précision de notre modèle, on utilise la fonction metrics.accuracy_score() entre la prédiction obtenue et le set de test qu’on était censé obtenir. Afin d’obtenir une plus large approximation du taux de réussite du modèle, on répète l’opération à l’aide d’une boucle for. La régression logistique atteint un taux de réussite d’environ 68,4 % et les SVM environ 68,1 %, tandis que l’arbre de décision n’atteint que 60,8 %. Bien que les programmes fonctionnent, les taux d’erreurs obtenues (<30%) restent trop importants pour quelque chose qui mérite autant de précision qu’un diagnostic d’ECG. Cela est potentiellement dû à la surabondance de patients par rapport au trop faible nombre de colonnes d’informations. Le programme a donc besoin d’être optimisé selon ces objectifs et c’est ce que j’ai cherché à faire dans la seconde version du programme.

6) Programme v2 :

Afin de connaître les colonnes qui ont du potentiel à être rajoutées, on calcule le nombre de cases vides dans chacune d’entre elles : avec une double boucle selon les différentes colonnes et leur taille, on vérifie si la taille des cases est strictement supérieur à zéro, si c’est le cas alors elle n’est pas vide. Les colonnes ‘nurse’ , ’heart_axis’ et ‘validated_by’ se distinguent des autres par leur nombre de cases non-vides ainsi que par le nombre de modifications à faire. La première et la troisième sont déjà des colonnes de valeurs numériques : on n’a pas besoin d’y apporter une quelconque modification. On réutilise les modifications faites aux colonnes dans le programme v1 et on rajoute la modification de la colonne ‘heart_axis’ en utilisant la même méthode que lors du programme v1. Le reste est identique au premier programme. On observe notre nouveau tableau final qui est de taille 761 lignes X 14 colonnes.L’arbre de décision atteint un taux de réussite d’environ 89,1 %, les SVM 90,5 % et la régression logistique obtient 92,2 %. On remarque alors une large augmentation par rapport au premier programme, réduisant drastiquement le taux d’erreur pour chaque modèle. La régression logistique apparaît donc comme le modèle le plus performant des 3 et ce pour les 2 programmes.

7) Conclusion :

On a réussit à créer un programme qui puisse prédire avec une certaine précision le diagnostic d’un ECG en utilisant le machine learning. Cependant, avec environ une erreur sur 10, le programme ne peut remplacer le diagnostic d’un vrai médecin car le domaine de la médecine mérite une plus grane précision. On a vu qu’il était possible d’augmenter le taux de précision drastiquement en apportant quelques modifications, donc il est tout à fait possible de faire mieux que nos résultats actuels soit en continuant à manipuler la PTB-XL ou alors en trouvant une base de données potentiellement plus fournie que celle qu’on a.
